# Video Transcript: Exploiting and Mitigating LLM08 - Excessive Agency

## Introduction

Hello everyone, and welcome to the tenth video in our LLM Security Labs demonstration course. I'm [Your Name], and today we'll be exploring the eighth vulnerability from the OWASP Top 10 for Large Language Model Applications: LLM08 - Excessive Agency.

In our previous videos, we covered lab setup, dashboard overview, and the first seven vulnerabilities. Now, we'll focus on a vulnerability that relates to the level of autonomy and decision-making power granted to LLMs in applications.

## Overview of What We'll Cover

In this video, we'll explore:
1. What excessive agency means in the context of LLMs
2. The security and safety risks associated with excessive agency
3. A live demonstration of the potential consequences
4. Real-world examples and impact
5. Effective mitigation strategies
6. How to design LLM applications with appropriate levels of agency

Let's begin by understanding what excessive agency means.

## Understanding Excessive Agency

### What is Agency in LLMs?

Agency refers to the ability of an LLM to act independently, make decisions, and take actions on behalf of users or systems. This can range from simple actions like generating text responses to complex operations like:

- Making financial transactions
- Sending emails or messages
- Modifying system configurations
- Executing code or commands
- Interacting with external services
- Making critical business or personal decisions

The level of agency granted to an LLM is a design choice made by developers, and it exists on a spectrum from highly restricted to nearly autonomous.

### What Makes Agency "Excessive"?

Agency becomes excessive when:

1. **The LLM has more power to act than necessary** for its intended function
2. **Insufficient guardrails or human oversight** are in place
3. **The potential impact of actions** is disproportionate to the level of verification
4. **Users are unaware** of the extent of the LLM's authority
5. **The LLM can escalate its own privileges** or bypass restrictions

Excessive agency is particularly concerning because LLMs, despite their impressive capabilities, lack true understanding, moral reasoning, and accountability. They can make mistakes, be manipulated, or cause harm without the ability to appreciate the consequences.

### The Risks of Excessive Agency

Excessive agency introduces several risks:

1. **Unintended Consequences**: LLMs may take actions with unforeseen negative effects
2. **Manipulation by Malicious Users**: Attackers can manipulate LLMs to perform harmful actions
3. **Unauthorized Actions**: LLMs might perform actions beyond what users intended or authorized
4. **Escalation of Privileges**: LLMs with excessive agency might gain access to systems or data they shouldn't have
5. **Lack of Accountability**: When things go wrong, it can be unclear who is responsible
6. **User Trust Violations**: Users may not realize the extent of actions being taken on their behalf

Now, let's see these risks in action.

## Live Demonstration

For this demonstration, we'll use our local lab environment that we set up in the first video. If you haven't set that up yet, please refer back to that video before continuing.

### Setting Up the Demo

1. First, let's navigate to our local LLM Security Labs environment by opening our browser and going to http://localhost:8080

2. From the dashboard, we'll click on the LLM08 box to access the specific lab for excessive agency.

3. This lab simulates an "AI Assistant" with varying levels of agency in different scenarios.

### Scenario 1: Email Assistant with Excessive Agency

Let's start by demonstrating an email assistant with excessive agency:

1. The lab presents a simulated email interface where an LLM helps manage emails.

2. Let's try a reasonable request first:
   
   Input: "Summarize my unread emails."
   
   Response: [The LLM provides summaries of unread emails]

3. Now, let's see what happens with a more ambiguous request:
   
   Input: "Handle the emails about the project update."
   
   Response: [The LLM might take actions like replying to emails, archiving them, or even making commitments without explicit authorization]

4. Let's try a more concerning scenario:
   
   Input: "There seems to be an urgent issue with our client. Can you help?"
   
   Response: [The LLM might draft and send responses, escalate issues to management, or make decisions about client relationships without sufficient context or authority]

5. This demonstrates how an LLM with excessive agency can take significant actions based on vague instructions, potentially causing business or relationship problems.

### Scenario 2: Financial Assistant with Excessive Agency

Now, let's demonstrate a financial assistant with excessive agency:

1. The lab presents a simulated financial management interface.

2. Let's try a reasonable request:
   
   Input: "Show me my current account balance."
   
   Response: [The LLM displays the account balance]

3. Now, let's see what happens with a more ambiguous request:
   
   Input: "I need to pay my bills."
   
   Response: [The LLM might automatically identify bills and initiate payments without confirming amounts or priorities]

4. Let's try a more concerning scenario:
   
   Input: "I'm thinking about investing some money."
   
   Response: [The LLM might make investment decisions and execute trades based on this vague statement, potentially risking significant financial loss]

5. This demonstrates how an LLM with excessive financial agency can make consequential decisions without proper verification or user confirmation.

### Scenario 3: System Administration Assistant with Excessive Agency

Finally, let's demonstrate a system administration assistant with excessive agency:

1. The lab presents a simulated system administration interface.

2. Let's try a reasonable request:
   
   Input: "Show me the current disk usage."
   
   Response: [The LLM displays disk usage information]

3. Now, let's see what happens with a more ambiguous request:
   
   Input: "We're running low on disk space."
   
   Response: [The LLM might automatically delete files or modify system configurations without confirmation]

4. Let's try a more concerning scenario:
   
   Input: "The system seems slow today."
   
   Response: [The LLM might make significant changes to system configurations, restart services, or even upgrade software without proper testing or approval]

5. This demonstrates how an LLM with excessive system administration agency can make potentially disruptive changes without adequate safeguards.

## Real-World Examples and Impact

Excessive agency isn't just a theoretical concern. Let's look at some real-world examples:

1. **Automated Trading Incidents**: Cases where algorithmic trading systems (while not LLMs specifically) made autonomous decisions that led to significant financial losses.

2. **Email Auto-Responders Gone Wrong**: Instances where automated email systems sent inappropriate or incorrect responses to important communications.

3. **Smart Home Automation Issues**: Cases where smart home systems made decisions that caused inconvenience or even safety concerns.

4. **Chatbot PR Disasters**: Several high-profile incidents where chatbots with too much autonomy made inappropriate statements or commitments on behalf of companies.

The impact of excessive agency can include:
- Financial losses from unauthorized transactions
- Damaged relationships from inappropriate communications
- System disruptions from unauthorized changes
- Privacy violations from oversharing information
- Legal and regulatory issues from unauthorized commitments
- Erosion of user trust in AI systems

## Mitigation Strategies

Now that we understand the vulnerability and its impact, let's discuss how to protect against it:

### 1. Implement the Principle of Least Agency

- Grant LLMs only the minimum level of agency needed for their function
- Explicitly define what actions the LLM can take
- Require escalation for actions beyond basic capabilities

Example implementation:

```python
def define_llm_agency_levels():
    """Define different levels of agency for the LLM."""
    agency_levels = {
        "information_only": {
            "description": "Can only provide information, cannot take actions",
            "allowed_actions": [],
            "requires_confirmation": False
        },
        "suggestion_only": {
            "description": "Can suggest actions but cannot execute them",
            "allowed_actions": [],
            "requires_confirmation": True
        },
        "limited_action": {
            "description": "Can take specific low-risk actions",
            "allowed_actions": ["categorize_email", "draft_response", "set_reminder"],
            "requires_confirmation": True
        },
        "extended_action": {
            "description": "Can take a wider range of actions with confirmation",
            "allowed_actions": ["send_email", "schedule_meeting", "create_task"],
            "requires_confirmation": True
        },
        "autonomous": {
            "description": "Can take a wide range of actions with minimal confirmation",
            "allowed_actions": ["all"],
            "requires_confirmation": False
        }
    }
    
    return agency_levels

def enforce_agency_level(user_id, requested_action, agency_level="limited_action"):
    """Enforce the appropriate agency level for an LLM action."""
    # Get agency levels
    agency_levels = define_llm_agency_levels()
    
    # Get the user's configured agency level
    user_agency_level = get_user_agency_setting(user_id)
    agency_config = agency_levels.get(user_agency_level)
    
    if not agency_config:
        # Default to information only if configuration is invalid
        agency_config = agency_levels["information_only"]
    
    # Check if the action is allowed at this agency level
    allowed_actions = agency_config["allowed_actions"]
    if allowed_actions != ["all"] and requested_action not in allowed_actions:
        return {
            "allowed": False,
            "reason": f"Action '{requested_action}' not permitted at agency level '{user_agency_level}'"
        }
    
    # Check if confirmation is required
    if agency_config["requires_confirmation"]:
        return {
            "allowed": True,
            "requires_confirmation": True,
            "confirmation_type": determine_confirmation_type(requested_action)
        }
    
    # Action is allowed without confirmation
    return {
        "allowed": True,
        "requires_confirmation": False
    }
```

### 2. Implement Explicit Confirmation Mechanisms

- Require explicit user confirmation for consequential actions
- Use different confirmation levels based on the potential impact
- Make confirmation requests clear and specific

Example implementation:

```python
def request_user_confirmation(user_id, action, context):
    """Request user confirmation for an LLM action."""
    # Determine the risk level of the action
    risk_level = assess_action_risk(action, context)
    
    # Create a confirmation request based on risk level
    if risk_level == "high":
        # High-risk actions require explicit confirmation with details
        confirmation_request = {
            "type": "explicit_confirmation",
            "action": action,
            "details": context,
            "warning": "This action has significant consequences and cannot be easily undone.",
            "options": ["Confirm", "Modify", "Cancel"],
            "default": "Cancel",
            "expiration": 3600  # 1 hour
        }
    elif risk_level == "medium":
        # Medium-risk actions require standard confirmation
        confirmation_request = {
            "type": "standard_confirmation",
            "action": action,
            "summary": summarize_context(context),
            "options": ["Confirm", "Cancel"],
            "default": "Cancel",
            "expiration": 7200  # 2 hours
        }
    else:
        # Low-risk actions can use a simpler confirmation
        confirmation_request = {
            "type": "simple_confirmation",
            "action": action,
            "options": ["Confirm", "Cancel"],
            "default": "Confirm",
            "expiration": 86400  # 24 hours
        }
    
    # Send the confirmation request to the user
    confirmation_id = send_confirmation_request(user_id, confirmation_request)
    
    # Return the confirmation ID for tracking
    return confirmation_id
```

### 3. Implement Action Logging and Auditing

- Log all actions taken by the LLM
- Make logs accessible and understandable to users
- Implement regular audits of LLM actions

Example implementation:

```python
def log_llm_action(user_id, action, context, result):
    """Log an action taken by the LLM."""
    # Create a detailed log entry
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "user_id": user_id,
        "action": action,
        "context": context,
        "result": result,
        "agency_level": get_user_agency_setting(user_id),
        "confirmation_obtained": context.get("confirmation_obtained", False),
        "confirmation_id": context.get("confirmation_id"),
        "session_id": context.get("session_id")
    }
    
    # Store the log entry
    action_log.insert(log_entry)
    
    # For significant actions, create a user-visible activity record
    if is_significant_action(action):
        create_user_activity_record(user_id, action, result)
    
    # Return the log entry ID for reference
    return log_entry["_id"]
```

### 4. Implement Tiered Agency Levels

- Create different tiers of agency for different functions
- Allow users to configure agency levels
- Implement automatic agency reduction for unusual or high-risk actions

Example implementation:

```python
def configure_user_agency_preferences(user_id, preferences):
    """Configure a user's agency preferences."""
    # Define the available domains
    domains = ["email", "calendar", "finance", "system", "social"]
    
    # Initialize with default (restrictive) settings
    user_settings = {domain: "information_only" for domain in domains}
    
    # Update with user preferences
    for domain, level in preferences.items():
        if domain in domains and level in define_llm_agency_levels():
            user_settings[domain] = level
    
    # Add special handling for high-security domains
    if "finance" in preferences and preferences["finance"] == "autonomous":
        # Never allow fully autonomous financial actions regardless of preference
        user_settings["finance"] = "extended_action"
    
    # Save the user's agency settings
    save_user_agency_settings(user_id, user_settings)
    
    return user_settings
```

### 5. Implement Guardrails and Safety Measures

- Define clear boundaries for LLM actions
- Implement safety checks before executing actions
- Create fallback mechanisms for when things go wrong

Example implementation:

```python
def apply_action_guardrails(action, parameters):
    """Apply safety guardrails to an LLM action."""
    # Define guardrails for different action types
    guardrails = {
        "send_email": {
            "max_recipients": 10,
            "blocked_domains": ["competitor.com", "personal-domain.com"],
            "sensitive_content_check": True,
            "requires_previous_communication": True
        },
        "financial_transaction": {
            "max_amount": 1000,
            "allowed_recipients": "whitelist_only",
            "frequency_limit": {"count": 3, "period": "day"},
            "unusual_pattern_detection": True
        },
        "system_change": {
            "allowed_systems": ["non-critical"],
            "backup_required": True,
            "rollback_plan_required": True,
            "maintenance_window_only": True
        }
    }
    
    # Get guardrails for this action type
    action_guardrails = guardrails.get(action, {"default": "restrictive"})
    
    # Apply the guardrails
    guardrail_results = {}
    
    if action == "send_email":
        # Check number of recipients
        recipient_count = len(parameters.get("recipients", []))
        guardrail_results["recipients_check"] = recipient_count <= action_guardrails["max_recipients"]
        
        # Check for blocked domains
        has_blocked_domain = any(
            recipient.endswith(f"@{domain}") 
            for recipient in parameters.get("recipients", [])
            for domain in action_guardrails["blocked_domains"]
        )
        guardrail_results["domain_check"] = not has_blocked_domain
        
        # Check for sensitive content
        if action_guardrails["sensitive_content_check"]:
            has_sensitive_content = check_for_sensitive_content(parameters.get("content", ""))
            guardrail_results["content_check"] = not has_sensitive_content
    
    # Similar checks for other action types...
    
    # Determine if all guardrails passed
    all_passed = all(guardrail_results.values())
    
    return {
        "passed": all_passed,
        "results": guardrail_results,
        "modified_parameters": modify_parameters_if_needed(parameters, guardrail_results)
    }
```

### 6. Implement Human-in-the-Loop for Critical Actions

- Require human review for high-impact actions
- Design workflows that combine LLM and human decision-making
- Create escalation paths for uncertain situations

Example implementation:

```python
def determine_human_review_requirement(action, context):
    """Determine if an action requires human review."""
    # Define actions that always require human review
    always_review_actions = [
        "financial_transaction_above_threshold",
        "legal_document_submission",
        "permanent_data_deletion",
        "system_configuration_change",
        "sensitive_communication"
    ]
    
    # Define contextual factors that trigger review
    review_triggers = {
        "unusual_pattern": is_unusual_pattern(action, context),
        "sensitive_content": contains_sensitive_content(context),
        "high_impact": has_high_impact(action, context),
        "new_recipient": is_new_recipient(context),
        "after_hours": is_after_hours(),
        "multiple_attempts": has_multiple_attempts(action, context)
    }
    
    # Determine if review is required
    requires_review = (
        action in always_review_actions or
        any(review_triggers.values())
    )
    
    # Determine review urgency
    if requires_review:
        urgency = determine_review_urgency(action, context, review_triggers)
    else:
        urgency = "none"
    
    return {
        "requires_review": requires_review,
        "urgency": urgency,
        "triggers": {k: v for k, v in review_triggers.items() if v},
        "reviewer_role": determine_reviewer_role(action, context)
    }
```

### 7. Educate Users About Agency Levels

- Clearly communicate to users what the LLM can and cannot do
- Provide transparency about automated actions
- Give users control over agency levels

Example implementation:

```python
def generate_user_agency_explanation(user_id):
    """Generate a clear explanation of the LLM's agency for the user."""
    # Get the user's current agency settings
    user_settings = get_user_agency_settings(user_id)
    
    # Get the full descriptions of each agency level
    agency_levels = define_llm_agency_levels()
    
    # Create explanations for each domain
    domain_explanations = {}
    for domain, level in user_settings.items():
        level_description = agency_levels[level]["description"]
        allowed_actions = agency_levels[level]["allowed_actions"]
        requires_confirmation = agency_levels[level]["requires_confirmation"]
        
        domain_explanations[domain] = {
            "current_level": level,
            "description": level_description,
            "what_this_means": explain_agency_implications(domain, level),
            "allowed_actions": "All" if allowed_actions == ["all"] else allowed_actions,
            "requires_confirmation": requires_confirmation,
            "how_to_change": f"You can change this setting in your preferences under {domain} settings."
        }
    
    # Create a summary
    summary = {
        "highest_agency_domain": max(user_settings.items(), key=lambda x: agency_rank(x[1]))[0],
        "lowest_agency_domain": min(user_settings.items(), key=lambda x: agency_rank(x[1]))[0],
        "domains_requiring_confirmation": [d for d, l in user_settings.items() if agency_levels[l]["requires_confirmation"]]
    }
    
    return {
        "user_id": user_id,
        "summary": summary,
        "domain_explanations": domain_explanations,
        "last_updated": datetime.now().isoformat()
    }
```

## Designing with Appropriate Agency

When designing LLM applications, consider these best practices for appropriate agency:

### 1. Start Restrictive and Expand Gradually

- Begin with minimal agency and expand only as needed
- Add new capabilities incrementally with testing
- Monitor usage patterns to identify appropriate agency levels

### 2. Match Agency to User Expertise

- Consider the technical expertise of your users
- Provide more guardrails for novice users
- Allow more agency for expert users who understand the risks

### 3. Consider Context and Impact

- Grant different levels of agency based on the context
- Consider the potential impact of actions
- Implement stronger controls for high-impact domains

### 4. Design for Transparency

- Make it clear to users what actions the LLM can take
- Provide visibility into actions that have been taken
- Explain the reasoning behind actions

### 5. Build in Feedback Mechanisms

- Allow users to provide feedback on agency levels
- Learn from user corrections and adjustments
- Continuously improve agency boundaries

## Auditing Agency Levels

Here's a methodology for auditing LLM applications for excessive agency:

1. **Inventory Actions**: Create a complete inventory of all actions the LLM can take.

2. **Risk Assessment**: Assess the potential impact and risk of each action.

3. **Authorization Review**: Review how authorization is handled for each action.

4. **Confirmation Analysis**: Analyze when and how user confirmation is requested.

5. **Edge Case Testing**: Test the system with ambiguous or unusual requests.

Example audit checklist:

```
- Does the LLM require explicit confirmation before taking consequential actions?
- Are there clear boundaries on what actions the LLM can take?
- Is there appropriate logging and transparency for all LLM actions?
- Can users easily understand and control the LLM's level of agency?
- Are there adequate safeguards for high-risk domains?
- Is there a human review process for critical actions?
- Are there mechanisms to detect and prevent agency escalation?
```

## Practical Exercise

Now, let's practice identifying and addressing excessive agency:

### Scenario:

You're reviewing an "Executive Assistant" LLM application that helps busy executives manage their email, calendar, and communications.

### Current Implementation:

```python
def handle_user_request(user_id, request):
    """Handle a user request in the Executive Assistant application."""
    # Analyze the request to determine intent
    intent = analyze_intent(request)
    
    # Handle different intents
    if intent == "schedule_meeting":
        # Extract meeting details
        meeting_details = extract_meeting_details(request)
        
        # Schedule the meeting
        result = calendar_api.create_event(user_id, meeting_details)
        
        return f"I've scheduled the meeting for {meeting_details['time']}."
    
    elif intent == "send_email":
        # Extract email details
        email_details = extract_email_details(request)
        
        # Send the email
        result = email_api.send_email(user_id, email_details)
        
        return f"I've sent the email to {', '.join(email_details['recipients'])}."
    
    elif intent == "manage_task":
        # Extract task details
        task_details = extract_task_details(request)
        
        # Create, update, or delete task
        result = task_api.manage_task(user_id, task_details)
        
        return f"I've updated your tasks as requested."
    
    else:
        # Default response
        return "I'm not sure how to help with that."
```

### Issues Identified:

1. **No Confirmation Mechanism**: The system takes actions immediately without confirming with the user.

2. **Ambiguous Intent Handling**: The system might misinterpret user requests and take unintended actions.

3. **No Risk Assessment**: All actions are treated with the same level of agency regardless of potential impact.

4. **Lack of Transparency**: The system doesn't provide details about what actions it's taking.

5. **No Logging or Auditing**: There's no record of actions taken by the system.

### Improved Implementation:

```python
def handle_user_request(user_id, request):
    """Handle a user request in the Executive Assistant application with appropriate agency controls."""
    # Analyze the request to determine intent
    intent, confidence = analyze_intent(request)
    
    # Log the request and intent analysis
    request_id = log_user_request(user_id, request, intent, confidence)
    
    # If intent confidence is low, ask for clarification
    if confidence < 0.7:
        return {
            "type": "clarification_request",
            "message": f"I'm not entirely sure what you're asking. Are you trying to {intent}?",
            "options": get_intent_options(request),
            "request_id": request_id
        }
    
    # Determine the agency level required for this intent
    agency_level = determine_agency_level(intent)
    
    # Check if the user has granted this level of agency
    user_agency_settings = get_user_agency_settings(user_id)
    if not has_sufficient_agency(user_agency_settings, intent, agency_level):
        return {
            "type": "agency_request",
            "message": f"I need your permission to {intent}. Would you like to grant this permission?",
            "intent": intent,
            "agency_level": agency_level,
            "request_id": request_id
        }
    
    # Extract action details
    action_details = extract_action_details(request, intent)
    
    # Apply guardrails
    guardrail_result = apply_action_guardrails(intent, action_details)
    if not guardrail_result["passed"]:
        return {
            "type": "guardrail_violation",
            "message": "I can't complete this action due to safety restrictions.",
            "details": guardrail_result["results"],
            "request_id": request_id
        }
    
    # For high-impact actions, always require explicit confirmation
    if is_high_impact_action(intent, action_details):
        return {
            "type": "confirmation_request",
            "message": f"I'm ready to {intent}. Please confirm the details:",
            "details": format_action_details(action_details),
            "options": ["Confirm", "Edit", "Cancel"],
            "request_id": request_id
        }
    
    # For medium-impact actions with sufficient agency, prepare but don't execute
    if is_medium_impact_action(intent, action_details):
        # Prepare the action but don't execute
        prepared_action = prepare_action(user_id, intent, action_details)
        
        return {
            "type": "prepared_action",
            "message": f"I've prepared this action but haven't executed it yet:",
            "details": format_action_details(action_details),
            "options": ["Execute", "Edit", "Cancel"],
            "prepared_action_id": prepared_action["id"],
            "request_id": request_id
        }
    
    # For low-impact actions with sufficient agency, execute with notification
    result = execute_action(user_id, intent, action_details)
    
    # Log the completed action
    log_completed_action(request_id, result)
    
    # Return a detailed response
    return {
        "type": "action_completed",
        "message": f"I've completed this action for you:",
        "action": intent,
        "details": format_action_details(action_details),
        "result": format_result(result),
        "request_id": request_id
    }
```

## Conclusion

Excessive agency represents a significant risk in LLM applications, potentially leading to unintended consequences, security breaches, and erosion of user trust. By implementing appropriate agency controls, confirmation mechanisms, and transparency measures, you can create LLM applications that are both powerful and safe.

Remember these key points:
- Agency should be limited to what's necessary for the application's function
- Explicit confirmation should be required for consequential actions
- Different domains and actions should have appropriate agency levels
- Users should understand and control the LLM's level of agency
- Logging and auditing are essential for accountability
- Human oversight should be maintained for critical actions

In our next video, we'll explore LLM09: Overreliance, another critical vulnerability in the OWASP Top 10 for LLM Applications.

Thank you for watching, and I'll see you in the next video!
