# Video Transcript: Exploiting and Mitigating LLM10 - Model Denial of Service

## Introduction

Hello everyone, and welcome to the twelfth and final video in our LLM Security Labs demonstration course. I'm [Your Name], and today we'll be exploring the tenth vulnerability from the OWASP Top 10 for Large Language Model Applications: LLM10 - Model Denial of Service.

In our previous videos, we covered lab setup, dashboard overview, and the first nine vulnerabilities. Now, we'll conclude our series by examining a vulnerability that can significantly impact the availability and performance of LLM systems.

## Overview of What We'll Cover

In this video, we'll explore:
1. What Model Denial of Service (DoS) attacks are in the context of LLMs
2. The different types and techniques of LLM DoS attacks
3. A live demonstration of LLM DoS attacks
4. Real-world examples and impact
5. Effective mitigation strategies
6. How to design LLM applications that are resilient to DoS attacks

Let's begin by understanding what Model Denial of Service means.

## Understanding Model Denial of Service

### What is Model Denial of Service?

Model Denial of Service (DoS) refers to attacks that aim to disrupt the availability, performance, or functionality of LLM systems. Unlike traditional DoS attacks that focus on network or infrastructure resources, Model DoS attacks specifically target the unique characteristics and vulnerabilities of large language models themselves.

These attacks can:
- Exhaust computational resources
- Degrade model performance
- Increase response latency
- Cause excessive token consumption
- Trigger system crashes or failures
- Lead to significant cost increases
- Reduce service availability for legitimate users

### Types of Model DoS Attacks

There are several types of Model DoS attacks:

1. **Resource Exhaustion Attacks**: These attacks aim to consume excessive computational resources by crafting inputs that require maximum processing power, memory, or time to process.

2. **Token Stuffing Attacks**: These attacks involve sending inputs with excessive tokens to consume token quotas or increase API costs.

3. **Recursive Expansion Attacks**: These attacks trick the model into generating increasingly large outputs through recursive patterns or self-referential prompts.

4. **Infinite Loop Induction**: These attacks attempt to create conditions where the model gets stuck in repetitive patterns or reasoning loops.

5. **Context Window Flooding**: These attacks fill the context window with irrelevant information to reduce the effective capacity for meaningful interaction.

6. **Jailbreak Amplification**: These attacks combine jailbreak techniques with DoS patterns to bypass restrictions while consuming excessive resources.

### Why Model DoS is Concerning

Model DoS attacks are particularly concerning for several reasons:

1. **Economic Impact**: LLM inference is computationally expensive, and DoS attacks can significantly increase operational costs.

2. **Service Degradation**: Attacks can reduce service quality for all users, not just the direct targets.

3. **Difficult Detection**: Some attacks can be subtle and difficult to distinguish from legitimate but resource-intensive requests.

4. **Evolving Techniques**: As LLMs evolve, new DoS vectors continue to emerge.

5. **Low Barrier to Entry**: Many DoS attacks require minimal technical expertise to execute.

Now, let's see these attacks in action.

## Live Demonstration

For this demonstration, we'll use our local lab environment that we set up in the first video. If you haven't set that up yet, please refer back to that video before continuing.

### Setting Up the Demo

1. First, let's navigate to our local LLM Security Labs environment by opening our browser and going to http://localhost:8080

2. From the dashboard, we'll click on the LLM10 box to access the specific lab for Model Denial of Service.

3. This lab provides us with a simulated LLM service that we can use to demonstrate various DoS attacks and their impacts.

### Demonstration 1: Token Stuffing Attack

Let's start by demonstrating a token stuffing attack:

1. The lab includes a simple chat interface with a token counter and response time metrics.

2. First, let's try a normal query to establish a baseline:
   
   Input: "What is the capital of France?"
   
   Response: "The capital of France is Paris."
   
   Metrics: 
   - Tokens used: 13
   - Response time: 0.2 seconds

3. Now, let's try a token stuffing attack:
   
   Input: [A very long input with thousands of characters, repeated text, or large blocks of irrelevant content]
   
   Response: [The model attempts to process this large input]
   
   Metrics:
   - Tokens used: 8,000+
   - Response time: 5+ seconds

4. This demonstrates how a malicious user could consume excessive tokens and computational resources with a single request.

### Demonstration 2: Recursive Expansion Attack

Now, let's demonstrate a recursive expansion attack:

1. This attack tricks the model into generating increasingly large outputs.

2. Let's try a recursive expansion prompt:
   
   Input: "Write a story. For each sentence in the story, expand it into a paragraph. For each paragraph, expand it into a page. Continue this pattern."
   
   Response: [The model begins generating increasingly verbose content until it hits token limits]
   
   Metrics:
   - Tokens used: Maximum output limit
   - Response time: 10+ seconds

3. This demonstrates how a carefully crafted prompt can cause the model to generate excessive output, consuming resources and potentially maxing out token limits.

### Demonstration 3: Infinite Loop Induction

Let's demonstrate an infinite loop induction attack:

1. This attack attempts to create conditions where the model gets stuck in repetitive patterns.

2. Let's try an infinite loop prompt:
   
   Input: "I'll give you a number, and you increment it by 1. Start with 1, and we'll continue this process, showing all previous numbers each time. 1."
   
   Response: [The model begins generating a sequence that grows with each step, potentially continuing until token limits are reached]
   
   Metrics:
   - Tokens used: Rapidly increasing
   - Response time: Increasing with each step

3. This demonstrates how a seemingly simple task can be designed to consume an ever-increasing amount of resources.

### Demonstration 4: Context Window Flooding

Finally, let's demonstrate a context window flooding attack:

1. This attack fills the context window with irrelevant information to reduce effective capacity.

2. Let's try a context flooding prompt:
   
   Input: [A prompt that fills most of the context window with irrelevant information, followed by a complex question that requires context]
   
   Response: [The model struggles to provide a coherent response due to limited effective context]
   
   Metrics:
   - Tokens used: Near maximum input limit
   - Response time: Extended due to processing large context
   - Response quality: Degraded due to limited effective context

3. This demonstrates how flooding the context window can degrade model performance and consume excessive resources.

## Real-World Examples and Impact

Model DoS isn't just a theoretical concern. Let's look at some real-world examples:

1. **API Cost Attacks**: Cases where malicious users have exploited token-based pricing models to generate excessive costs for service providers.

2. **Service Disruptions**: Instances where targeted DoS attacks have caused degraded performance or temporary unavailability of commercial LLM services.

3. **Resource Contention**: Situations where DoS attacks on shared infrastructure have affected service quality for all users.

4. **Competitive Disruption**: Cases where competitors have allegedly used DoS techniques to degrade rival services.

The impact of these attacks can include:
- Significant financial costs due to pay-per-token or compute-based pricing models
- Reduced service availability for legitimate users
- Degraded model performance and response quality
- Loss of user trust and satisfaction
- Operational disruptions for businesses relying on LLM services

## Mitigation Strategies

Now that we understand the vulnerability and its impact, let's discuss how to protect against it:

### 1. Implement Rate Limiting and Quotas

- Limit the number of requests per user or API key
- Implement token consumption quotas
- Use tiered access levels with appropriate limits

Example implementation:

```python
def apply_rate_limiting(user_id, request_type):
    """Apply rate limiting based on user and request type."""
    # Define rate limits for different request types
    rate_limits = {
        "standard_query": {
            "requests_per_minute": 10,
            "tokens_per_minute": 10000,
            "max_tokens_per_request": 1000
        },
        "complex_query": {
            "requests_per_minute": 5,
            "tokens_per_minute": 20000,
            "max_tokens_per_request": 4000
        },
        "batch_processing": {
            "requests_per_minute": 2,
            "tokens_per_minute": 50000,
            "max_tokens_per_request": 8000
        }
    }
    
    # Get the appropriate limits
    limits = rate_limits.get(request_type, rate_limits["standard_query"])
    
    # Check if the user has exceeded request rate
    current_rpm = get_current_request_rate(user_id)
    if current_rpm >= limits["requests_per_minute"]:
        return {
            "allowed": False,
            "reason": "Request rate limit exceeded",
            "retry_after": calculate_retry_after(user_id, "requests")
        }
    
    # Check if the user has exceeded token rate
    current_tpm = get_current_token_rate(user_id)
    if current_tpm >= limits["tokens_per_minute"]:
        return {
            "allowed": False,
            "reason": "Token rate limit exceeded",
            "retry_after": calculate_retry_after(user_id, "tokens")
        }
    
    # All checks passed
    return {
        "allowed": True,
        "limits": limits
    }
```

### 2. Implement Input Validation and Sanitization

- Validate input length and complexity
- Remove or limit repetitive patterns
- Detect and reject known DoS patterns

Example implementation:

```python
def validate_llm_input(input_text):
    """Validate and sanitize input to prevent DoS attacks."""
    validation_results = {
        "is_valid": True,
        "issues": [],
        "sanitized_input": input_text
    }
    
    # Check input length
    if len(input_text) > MAX_INPUT_LENGTH:
        validation_results["is_valid"] = False
        validation_results["issues"].append("Input exceeds maximum length")
        validation_results["sanitized_input"] = input_text[:MAX_INPUT_LENGTH]
    
    # Check for excessive repetition
    repetition_score = calculate_repetition_score(input_text)
    if repetition_score > REPETITION_THRESHOLD:
        validation_results["is_valid"] = False
        validation_results["issues"].append("Input contains excessive repetition")
        validation_results["sanitized_input"] = reduce_repetition(input_text)
    
    # Check for known DoS patterns
    dos_patterns = load_dos_patterns()
    for pattern_name, pattern in dos_patterns.items():
        if pattern.search(input_text):
            validation_results["is_valid"] = False
            validation_results["issues"].append(f"Input matches known DoS pattern: {pattern_name}")
            validation_results["sanitized_input"] = pattern.remove(validation_results["sanitized_input"])
    
    # Check entropy (too low entropy often indicates DoS attempts)
    entropy = calculate_entropy(input_text)
    if entropy < MIN_ENTROPY_THRESHOLD:
        validation_results["is_valid"] = False
        validation_results["issues"].append("Input has suspiciously low entropy")
    
    return validation_results
```

### 3. Implement Output Controls

- Set appropriate maximum output token limits
- Monitor and limit recursive or expanding outputs
- Implement early stopping for problematic generation patterns

Example implementation:

```python
def configure_output_controls(user_id, input_analysis):
    """Configure output controls based on user and input analysis."""
    # Base output controls
    output_controls = {
        "max_tokens": DEFAULT_MAX_TOKENS,
        "early_stopping": True,
        "repetition_penalty": 1.0,
        "output_filtering": "standard"
    }
    
    # Adjust based on user tier
    user_tier = get_user_tier(user_id)
    if user_tier == "premium":
        output_controls["max_tokens"] = PREMIUM_MAX_TOKENS
    elif user_tier == "enterprise":
        output_controls["max_tokens"] = ENTERPRISE_MAX_TOKENS
    
    # Adjust based on input risk analysis
    risk_level = input_analysis.get("risk_level", "medium")
    
    if risk_level == "high":
        # For high-risk inputs, apply stricter controls
        output_controls["max_tokens"] = min(output_controls["max_tokens"], HIGH_RISK_MAX_TOKENS)
        output_controls["repetition_penalty"] = 1.3
        output_controls["output_filtering"] = "aggressive"
    elif risk_level == "medium":
        # For medium-risk inputs, apply moderate controls
        output_controls["repetition_penalty"] = 1.1
        output_controls["output_filtering"] = "moderate"
    
    # Add dynamic monitoring for potentially problematic outputs
    output_controls["monitor_expansion_ratio"] = True
    output_controls["max_expansion_ratio"] = 5.0  # Output should not be more than 5x input size
    
    return output_controls
```

### 4. Implement Resource Consumption Monitoring

- Monitor token usage patterns for anomalies
- Track computational resource consumption
- Implement alerts for unusual patterns

Example implementation:

```python
def monitor_resource_consumption(request_id, user_id, input_stats, output_stats, performance_metrics):
    """Monitor resource consumption for anomalies."""
    # Record the consumption data
    consumption_record = {
        "timestamp": datetime.now().isoformat(),
        "request_id": request_id,
        "user_id": user_id,
        "input_tokens": input_stats["token_count"],
        "output_tokens": output_stats["token_count"],
        "total_tokens": input_stats["token_count"] + output_stats["token_count"],
        "processing_time": performance_metrics["processing_time"],
        "cpu_usage": performance_metrics["cpu_usage"],
        "memory_usage": performance_metrics["memory_usage"],
        "expansion_ratio": output_stats["token_count"] / max(1, input_stats["token_count"])
    }
    
    # Save the consumption record
    save_consumption_record(consumption_record)
    
    # Check for anomalies
    anomalies = []
    
    # Check for unusually high token consumption
    user_avg_tokens = get_user_average_token_usage(user_id)
    if consumption_record["total_tokens"] > user_avg_tokens * 3:
        anomalies.append({
            "type": "high_token_consumption",
            "severity": "medium",
            "details": f"Token usage {consumption_record['total_tokens']} is {consumption_record['total_tokens'] / user_avg_tokens:.1f}x user average"
        })
    
    # Check for unusually high expansion ratio
    if consumption_record["expansion_ratio"] > 10:
        anomalies.append({
            "type": "high_expansion_ratio",
            "severity": "high",
            "details": f"Output is {consumption_record['expansion_ratio']:.1f}x larger than input"
        })
    
    # Check for unusually long processing time
    user_avg_time = get_user_average_processing_time(user_id)
    if consumption_record["processing_time"] > user_avg_time * 5:
        anomalies.append({
            "type": "long_processing_time",
            "severity": "medium",
            "details": f"Processing time {consumption_record['processing_time']:.2f}s is {consumption_record['processing_time'] / user_avg_time:.1f}x user average"
        })
    
    # If anomalies were detected, trigger alerts
    if anomalies:
        trigger_consumption_anomaly_alert(user_id, request_id, anomalies)
    
    return {
        "consumption_record": consumption_record,
        "anomalies": anomalies
    }
```

### 5. Implement Cost Control Mechanisms

- Set budget limits for token consumption
- Implement graduated throttling as limits are approached
- Provide visibility into resource consumption

Example implementation:

```python
def apply_cost_controls(user_id, estimated_cost):
    """Apply cost controls based on user settings and estimated request cost."""
    # Get user's cost control settings
    cost_settings = get_user_cost_settings(user_id)
    
    # Get user's current usage for the billing period
    current_usage = get_current_billing_period_usage(user_id)
    
    # Check if this request would exceed the budget
    if cost_settings["budget_limit"] > 0:
        if current_usage["total_cost"] + estimated_cost > cost_settings["budget_limit"]:
            if cost_settings["exceed_action"] == "block":
                return {
                    "allowed": False,
                    "reason": "Budget limit would be exceeded",
                    "current_usage": current_usage,
                    "estimated_cost": estimated_cost,
                    "budget_limit": cost_settings["budget_limit"]
                }
            elif cost_settings["exceed_action"] == "warn":
                # Allow but log a warning
                log_budget_warning(user_id, current_usage, estimated_cost)
    
    # Check if approaching budget limit and apply graduated throttling
    if cost_settings["budget_limit"] > 0:
        usage_percentage = (current_usage["total_cost"] + estimated_cost) / cost_settings["budget_limit"]
        
        if usage_percentage > 0.9:
            # Over 90% of budget, apply strong throttling
            return {
                "allowed": True,
                "throttle_level": "high",
                "reason": "Over 90% of budget limit",
                "current_usage": current_usage,
                "estimated_cost": estimated_cost
            }
        elif usage_percentage > 0.7:
            # Over 70% of budget, apply moderate throttling
            return {
                "allowed": True,
                "throttle_level": "medium",
                "reason": "Over 70% of budget limit",
                "current_usage": current_usage,
                "estimated_cost": estimated_cost
            }
    
    # No budget concerns
    return {
        "allowed": True,
        "throttle_level": "none",
        "current_usage": current_usage,
        "estimated_cost": estimated_cost
    }
```

### 6. Implement Tiered Service Levels

- Create different service tiers with appropriate limits
- Implement graceful degradation under load
- Prioritize critical workloads

Example implementation:

```python
def apply_service_tier_controls(user_id, request_type, system_load):
    """Apply controls based on service tier and system load."""
    # Get user's service tier
    user_tier = get_user_service_tier(user_id)
    
    # Define tier-specific controls
    tier_controls = {
        "free": {
            "max_requests_per_day": 50,
            "max_tokens_per_request": 1000,
            "max_concurrent_requests": 1,
            "priority_level": 0,
            "load_shedding_threshold": 0.7  # Start shedding at 70% system load
        },
        "basic": {
            "max_requests_per_day": 200,
            "max_tokens_per_request": 2000,
            "max_concurrent_requests": 2,
            "priority_level": 1,
            "load_shedding_threshold": 0.8
        },
        "premium": {
            "max_requests_per_day": 1000,
            "max_tokens_per_request": 4000,
            "max_concurrent_requests": 5,
            "priority_level": 2,
            "load_shedding_threshold": 0.9
        },
        "enterprise": {
            "max_requests_per_day": 10000,
            "max_tokens_per_request": 8000,
            "max_concurrent_requests": 20,
            "priority_level": 3,
            "load_shedding_threshold": 0.95
        }
    }
    
    # Get controls for this tier
    controls = tier_controls.get(user_tier, tier_controls["free"])
    
    # Check daily request limit
    daily_requests = get_daily_request_count(user_id)
    if daily_requests >= controls["max_requests_per_day"]:
        return {
            "allowed": False,
            "reason": "Daily request limit exceeded",
            "current_count": daily_requests,
            "limit": controls["max_requests_per_day"]
        }
    
    # Check concurrent request limit
    concurrent_requests = get_concurrent_request_count(user_id)
    if concurrent_requests >= controls["max_concurrent_requests"]:
        return {
            "allowed": False,
            "reason": "Concurrent request limit exceeded",
            "current_count": concurrent_requests,
            "limit": controls["max_concurrent_requests"]
        }
    
    # Check system load for load shedding
    if system_load > controls["load_shedding_threshold"]:
        # Only allow if this is a high-priority request type or high-tier user
        if request_type != "high_priority" and controls["priority_level"] < 2:
            return {
                "allowed": False,
                "reason": "System under heavy load, temporarily unavailable for this tier",
                "retry_after": estimate_load_reduction_time(system_load)
            }
    
    # All checks passed
    return {
        "allowed": True,
        "tier": user_tier,
        "limits": {
            "max_tokens_per_request": controls["max_tokens_per_request"],
            "remaining_daily_requests": controls["max_requests_per_day"] - daily_requests
        }
    }
```

### 7. Implement Circuit Breakers

- Detect and stop runaway processes
- Implement timeouts for model inference
- Create fallback mechanisms for degraded operations

Example implementation:

```python
def configure_circuit_breakers(request_context):
    """Configure circuit breakers to prevent runaway processes."""
    # Define circuit breaker configurations
    circuit_breakers = {
        # Time-based circuit breaker
        "timeout": {
            "enabled": True,
            "threshold": 10.0,  # seconds
            "action": "terminate_gracefully"
        },
        
        # Token expansion circuit breaker
        "expansion_ratio": {
            "enabled": True,
            "threshold": 20.0,  # output/input token ratio
            "action": "terminate_with_warning"
        },
        
        # Repetition circuit breaker
        "repetition": {
            "enabled": True,
            "threshold": 0.7,  # repetition score
            "action": "terminate_with_warning"
        },
        
        # Resource usage circuit breaker
        "resource_usage": {
            "enabled": True,
            "cpu_threshold": 0.9,  # 90% CPU utilization
            "memory_threshold": 0.85,  # 85% memory utilization
            "action": "throttle_then_terminate"
        }
    }
    
    # Adjust based on request context
    if request_context.get("is_batch_job", False):
        # Batch jobs can run longer
        circuit_breakers["timeout"]["threshold"] = 30.0
    
    if request_context.get("tier") == "enterprise":
        # Enterprise tier gets more resources
        circuit_breakers["resource_usage"]["cpu_threshold"] = 0.95
        circuit_breakers["resource_usage"]["memory_threshold"] = 0.9
    
    if request_context.get("request_type") == "summarization":
        # Summarization can have higher expansion ratios
        circuit_breakers["expansion_ratio"]["threshold"] = 0.5  # output should be smaller than input
    
    return circuit_breakers
```

## Designing DoS-Resistant LLM Applications

When designing LLM applications, consider these best practices to improve resistance to DoS attacks:

### 1. Defense-in-Depth Approach

- Implement multiple layers of protection
- Combine preventive and detective controls
- Create fallback mechanisms for when primary defenses fail

### 2. Capacity Planning and Scaling

- Design systems to scale under increased load
- Implement load balancing across multiple instances
- Use cloud auto-scaling capabilities when possible

### 3. Economic Models

- Design pricing and usage models that disincentivize abuse
- Consider token-based pricing with appropriate limits
- Implement deposits or pre-payment for high-risk users

### 4. Monitoring and Alerting

- Implement comprehensive monitoring of resource usage
- Create alerts for unusual patterns or spikes
- Develop playbooks for responding to DoS incidents

### 5. Regular Testing

- Conduct regular DoS testing to identify vulnerabilities
- Update defenses based on emerging attack patterns
- Train response teams on DoS scenarios

## Auditing for DoS Vulnerabilities

Here's a methodology for auditing LLM applications for DoS vulnerabilities:

1. **Input Handling Review**: Examine how the application validates and processes user inputs.

2. **Resource Limit Analysis**: Assess what limits are in place and whether they're appropriate.

3. **Load Testing**: Test the system's behavior under high load conditions.

4. **Attack Simulation**: Attempt common DoS attack patterns in a controlled environment.

5. **Monitoring Review**: Evaluate the effectiveness of monitoring and alerting systems.

Example audit checklist:

```
- Are there appropriate input validation controls?
- Are there token limits for both input and output?
- Is there rate limiting at the user and API levels?
- Are there mechanisms to detect and prevent recursive expansion?
- Is there monitoring for unusual resource consumption?
- Are there circuit breakers to stop runaway processes?
- Are there cost control mechanisms?
- Is there appropriate logging of resource usage?
- Are there fallback mechanisms for degraded operations?
```

## Practical Exercise

Now, let's practice identifying and addressing DoS vulnerabilities:

### Scenario:

You're reviewing an "AI Content Generator" application that uses an LLM to generate marketing content for businesses.

### Current Implementation:

```python
def generate_content(prompt, max_length=None):
    """Generate marketing content based on a prompt."""
    # Prepare the request
    request = {
        "prompt": prompt,
        "max_tokens": max_length if max_length else 4000,
        "temperature": 0.7
    }
    
    # Call the LLM API
    response = llm_api.generate(request)
    
    # Return the generated content
    return response["text"]

# API endpoint
@app.route('/api/generate', methods=['POST'])
def api_generate_content():
    data = request.json
    prompt = data.get('prompt', '')
    max_length = data.get('max_length', None)
    
    # Generate the content
    generated_content = generate_content(prompt, max_length)
    
    return jsonify({
        "content": generated_content
    })
```

### Issues Identified:

1. **No Input Validation**: The application doesn't validate or limit the prompt length.

2. **Unrestricted Output Length**: If no max_length is provided, it defaults to a high value (4000 tokens).

3. **No Rate Limiting**: There are no limits on how many requests a user can make.

4. **No Resource Monitoring**: The application doesn't monitor resource usage or detect anomalies.

5. **No Cost Controls**: There are no mechanisms to prevent excessive token consumption.

### Improved Implementation:

```python
def validate_content_request(prompt, max_length, user_id):
    """Validate a content generation request."""
    validation_result = {
        "is_valid": True,
        "issues": [],
        "sanitized_prompt": prompt,
        "adjusted_max_length": max_length
    }
    
    # Check prompt length
    if len(prompt) > MAX_PROMPT_LENGTH:
        validation_result["is_valid"] = False
        validation_result["issues"].append(f"Prompt exceeds maximum length of {MAX_PROMPT_LENGTH} characters")
        validation_result["sanitized_prompt"] = prompt[:MAX_PROMPT_LENGTH]
    
    # Check for DoS patterns
    dos_check = check_for_dos_patterns(prompt)
    if dos_check["detected"]:
        validation_result["is_valid"] = False
        validation_result["issues"].append(f"Prompt contains potential DoS pattern: {dos_check['pattern_type']}")
        validation_result["sanitized_prompt"] = dos_check["sanitized_prompt"]
    
    # Validate and adjust max_length
    user_tier = get_user_tier(user_id)
    tier_max_length = TIER_MAX_LENGTHS.get(user_tier, DEFAULT_MAX_LENGTH)
    
    if max_length is None or max_length > tier_max_length:
        validation_result["adjusted_max_length"] = tier_max_length
        if max_length is not None and max_length > tier_max_length:
            validation_result["issues"].append(f"Requested length exceeds tier maximum of {tier_max_length}")
    
    return validation_result

def check_rate_limits(user_id):
    """Check if the user has exceeded rate limits."""
    # Get user's rate limit configuration
    user_tier = get_user_tier(user_id)
    rate_limits = TIER_RATE_LIMITS.get(user_tier, DEFAULT_RATE_LIMITS)
    
    # Check requests per minute
    rpm = get_requests_per_minute(user_id)
    if rpm >= rate_limits["requests_per_minute"]:
        return {
            "allowed": False,
            "reason": "Rate limit exceeded",
            "current": rpm,
            "limit": rate_limits["requests_per_minute"],
            "reset_after": get_rate_limit_reset_time(user_id, "rpm")
        }
    
    # Check daily request limit
    daily_requests = get_daily_requests(user_id)
    if daily_requests >= rate_limits["daily_requests"]:
        return {
            "allowed": False,
            "reason": "Daily request limit exceeded",
            "current": daily_requests,
            "limit": rate_limits["daily_requests"],
            "reset_after": get_rate_limit_reset_time(user_id, "daily")
        }
    
    # Check token consumption rate
    token_rate = get_token_consumption_rate(user_id)
    if token_rate >= rate_limits["tokens_per_minute"]:
        return {
            "allowed": False,
            "reason": "Token consumption rate exceeded",
            "current": token_rate,
            "limit": rate_limits["tokens_per_minute"],
            "reset_after": get_rate_limit_reset_time(user_id, "tokens")
        }
    
    return {
        "allowed": True,
        "limits": rate_limits,
        "usage": {
            "requests_per_minute": rpm,
            "daily_requests": daily_requests,
            "tokens_per_minute": token_rate
        }
    }

def generate_content(prompt, max_length, user_id):
    """Generate marketing content with DoS protections."""
    # Start tracking resource usage
    tracking_id = start_resource_tracking(user_id)
    
    try:
        # Validate the request
        validation = validate_content_request(prompt, max_length, user_id)
        if not validation["is_valid"]:
            log_validation_issues(user_id, validation["issues"])
        
        sanitized_prompt = validation["sanitized_prompt"]
        adjusted_max_length = validation["adjusted_max_length"]
        
        # Check rate limits
        rate_check = check_rate_limits(user_id)
        if not rate_check["allowed"]:
            return {
                "error": rate_check["reason"],
                "details": rate_check,
                "retry_after": rate_check["reset_after"]
            }
        
        # Configure circuit breakers
        circuit_breakers = configure_circuit_breakers({
            "user_id": user_id,
            "tier": get_user_tier(user_id),
            "request_type": "content_generation"
        })
        
        # Prepare the request with appropriate limits
        request = {
            "prompt": sanitized_prompt,
            "max_tokens": adjusted_max_length,
            "temperature": 0.7,
            "timeout": circuit_breakers["timeout"]["threshold"],
            "safety_settings": get_safety_settings(user_id)
        }
        
        # Call the LLM API with monitoring
        response = llm_api.generate_with_monitoring(request, circuit_breakers)
        
        # Track token usage
        token_usage = {
            "input_tokens": response["usage"]["input_tokens"],
            "output_tokens": response["usage"]["output_tokens"],
            "total_tokens": response["usage"]["total_tokens"]
        }
        update_token_usage(user_id, token_usage)
        
        # Check for anomalies in the response
        anomaly_check = check_for_response_anomalies(
            prompt=sanitized_prompt,
            response=response["text"],
            token_usage=token_usage
        )
        
        if anomaly_check["anomalies_detected"]:
            log_response_anomalies(user_id, anomaly_check["anomalies"])
        
        # Return the generated content with usage information
        return {
            "content": response["text"],
            "usage": token_usage,
            "warnings": validation["issues"] + anomaly_check.get("warnings", [])
        }
    
    finally:
        # Stop resource tracking and record metrics
        resource_metrics = stop_resource_tracking(tracking_id)
        log_resource_usage(user_id, resource_metrics)

# API endpoint with DoS protections
@app.route('/api/generate', methods=['POST'])
def api_generate_content():
    # Get request data
    data = request.json
    prompt = data.get('prompt', '')
    max_length = data.get('max_length', None)
    
    # Get user ID from authentication
    user_id = get_authenticated_user_id(request)
    
    # Apply global rate limiting
    if not global_rate_limiter.allow_request(request.remote_addr):
        return jsonify({
            "error": "Too many requests",
            "retry_after": global_rate_limiter.get_retry_after(request.remote_addr)
        }), 429
    
    # Generate the content with protections
    result = generate_content(prompt, max_length, user_id)
    
    # Check if there was an error
    if "error" in result:
        status_code = 429 if "rate" in result["error"].lower() else 400
        return jsonify(result), status_code
    
    # Return the generated content
    return jsonify(result)
```

## Conclusion

Model Denial of Service represents a significant risk for LLM applications, potentially leading to service disruptions, increased costs, and degraded user experience. By implementing appropriate input validation, rate limiting, resource monitoring, and circuit breakers, you can create LLM applications that are resilient to DoS attacks.

Remember these key points:
- LLM DoS attacks target the unique characteristics of language models
- Input validation and sanitization are critical first lines of defense
- Rate limiting and quotas help prevent resource exhaustion
- Monitoring and alerting enable early detection of attacks
- Circuit breakers prevent runaway processes
- A defense-in-depth approach provides the best protection

This concludes our series on the OWASP Top 10 for Large Language Model Applications. Throughout these videos, we've explored the unique security challenges posed by LLMs and demonstrated practical techniques for identifying and mitigating these vulnerabilities.

As LLM technology continues to evolve, so too will the security landscape. By applying the principles and techniques we've discussed in this course, you'll be well-equipped to build secure and resilient LLM applications.

Thank you for joining me on this journey through LLM security. I hope you've found these videos informative and practical. Remember to apply these security principles in your own LLM projects, and stay vigilant as new vulnerabilities and attack vectors emerge.

Until next time, this is [Your Name] signing off. Stay secure!
